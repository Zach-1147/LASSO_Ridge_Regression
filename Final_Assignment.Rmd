---
title: "Final_Assignment"
author: "Leon Edmiidz, Zach Ribau"
date: "2024-02-20"
output: pdf_document
---

```{r}

##----------------------
### Load Libraries & Data
##-----------------------

#Define function to install libraries if not already installed on system
install_if_needed <- function(package_name) {
  if (!require(package_name, character.only = TRUE)) {
    install.packages(package_name)
    library(package_name, character.only = TRUE)
  }
}

#Create vector with required libraries

packages <- c("tidyverse","glmnet", "ggplot2","lars", "caret", "ggplot2", "GGally", "knitr", "gridExtra")

#Loop over package vector and install and load libraries
for (pkg in packages) {
  install_if_needed(pkg)
}

#load in diabetes dataset
data("diabetes")
attach(diabetes)

##----------------------
### Data Splitting
##----------------------

# --- Problem 1 - a) ----

x <- diabetes$x2

#Set seed for reproducibility, once at beginning of script
set.seed(1545)

#Hold out 42 observations for testing..)
test_set_size <- 42 
test.index <- sample.int(dim(x)[1], test_set_size, replace = FALSE)


#Check for standardization, even though glmnet inherently standardizes.
plot(as.ts(apply(x, 2, sd)), xlab="Covariate", ylab="Standard Deviation") 

#Subset training data
x_training <- x[-test.index, ]
y_training <- y[-test.index]

#Subset the testing data
x_testing <- x[test.index, ]
y_testing <- y[test.index]


##-------------------##
### Cross Validation ###
##-------------------##

# --- Problem 1 - b) - i) ---- Performing cross validation with training data

#Perform 10-fold cross-validation for LASSO model using MSE as measure, 10 folds, and setting keep as true to retain fold id's for consistency throughout the script.
lassoCV <- cv.glmnet(x_training, y_training, type.measure="mse", nfolds = 10, keep = TRUE, alpha =1)

#Save the fold IDs to a variable to set later
std_foldid <- lassoCV$foldid

#Plot the cross validation results, showing mean MSE versus lambda.
par(mar=c(5, 4, 6, 2))
plot(lassoCV)
title("LASSO Cross-Validation: MSE vs Lambda")

# --- Problem 1 - b) - ii) Compare alternative models in sps.set ----

#Display a table containing alternative models, chosen as those with regularization terms between lambda min and lambda 1se. Show the number of non zero coefficients as well. It will be saved as a dataframe for manual investigation

min_mse <- min(lassoCV$cvm) #Save min mse for formula used below

sps.set_df <- data.frame(lambda = lassoCV$lambda,nzero = lassoCV$nzero,mse = lassoCV$cvm) %>%
  filter(lambda >= lassoCV$lambda.min & lambda <= lassoCV$lambda.1se) %>%
  arrange(mse) %>%
mutate(percent_diff = ((mse - min_mse) /min_mse) * 100) #Add column to show percentage difference for each MSE from the best model (lambda.min)

#Plot the change in mean MSE as a percentage of the minimum MSE specifically for our set of sps.set_df models, looking also at the number of nonzero coefficients (set to point size).

sps.set_trainPlot <- ggplot(sps.set_df, aes(x = log(lambda), y = percent_diff)) +
  geom_point(aes(size = nzero), alpha = 0.75) + 
  geom_line() +
  geom_point(data = sps.set_df[9, ], color = "blue", size = 5, shape = 8, alpha = 0.6) + #Highlight alternative model choice
  geom_text(aes(label = nzero), vjust = -0.8, hjust = 2, color = "darkgrey", size = 3.5) +
  geom_text(data = sps.set_df[9, ], aes(label = sprintf("lambda = %.6f", lambda)), vjust = 0,hjust = 1.5, color = "blue") +
  scale_size_continuous(range = c(1, 6), guide = "none") + #Adjust point sizes based on nzero
  scale_y_continuous(breaks = function(x) pretty(x, n = 10, min.n = 2)) +
  theme_minimal() +
labs(title = "MSE % Change vs. Log(λ) for LASSO CV Training sps.set Models",
     x = "Log(Lambda)",
     y = "% Difference from Lambda.min MSE")

sps.set_trainPlot 

#Set our sps.set model lambda's to a numeric vector 
sps.set <- sps.set_df$lambda

##-------------------------------
### Testing & Model Evaluations
##------------------------------

# --- Problem 1 - b) - iii) Predict response variable and evaluate models  ----

#Predict the response variables from the testing dataset
prd.sps.set <- predict(lassoCV, newx = x_testing, s = sps.set)

#We should get predictions for each of the 9 models, across 42 observations.
dim(prd.sps.set)

#Now we will compute the MSE for all of our models by comparing to the ground truth disease progression values

#Defining the function shown in class
mse <- function (i,y) {
  mean ( (y-i)^2 ) }	

#Apply the function to the prediction matrix and the test set response variables
mse.sps.set <- apply(prd.sps.set,2,mse,y_testing)

min_mse_test <- min(mse.sps.set) #Save min mse for formula below

#Store summary stats in a data frame again to investigate
sps.set_testdf <- data.frame(
  lambda = sps.set,
  mse = mse.sps.set,
  nzero = sps.set_df$nzero,
  percent_diff = ((mse.sps.set - min_mse_test) / min_mse_test) * 100)

#Create another plot of mean MSE % over the lambda min, versus lambda values, but this time for sps.set testing results.

sps.set_testPlot <- ggplot(sps.set_testdf, aes(x = log(lambda), y = percent_diff)) +
  geom_point(aes(size = nzero), alpha = 0.75) +  # Size mapping reintroduced with nzero
  geom_line() +
  geom_point(data = sps.set_testdf[9, ], color = "orange", size = 5, shape = 8, alpha = 0.6) + #Highlight alternative model choice
  geom_text(aes(label = nzero), vjust = -0.8, hjust = 2, color = "darkgrey", size = 3.5) +  # nzero labels reintroduced
  geom_text(data = sps.set_testdf[9, ], aes(label = sprintf("lambda = %.6f", lambda)), vjust = 0, hjust = 1.2, color = "orange") +
  scale_size_continuous(range = c(1, 6), guide = "none") +
  scale_y_continuous(breaks = function(x) pretty(x, n = 10, min.n = 2)) + 
  theme_minimal() +
  labs(title = "MSE % Change vs. Log(λ) for Testing sps.set Models",
       x = "Log(Lambda)",
       y = "% Difference from Lambda.min MSE")

sps.set_testPlot

#We can combine the lines from the training and testing % change MSE plots to look at the behaviour of the model error in testing and training compared to the best model

combinedPlot <- ggplot() +
  #Training data - Blue
  geom_line(data = sps.set_df, aes(x = log(lambda), y = percent_diff, group = 1), color = "blue") +
  geom_point(data = sps.set_df, aes(x = log(lambda), y = percent_diff), alpha = 0.75, color = "blue", size = 2) +
  geom_point(data = sps.set_df[9, ], aes(x = log(lambda), y = percent_diff), color = "blue", size = 5, shape = 8, alpha = 0.6) +
  
  #Testing data - Orange
  geom_line(data = sps.set_testdf, aes(x = log(lambda), y = percent_diff, group = 1), color = "orange") +
  geom_point(data = sps.set_testdf, aes(x = log(lambda), y = percent_diff), alpha = 0.75, color = "orange", size = 2) +
  geom_point(data = sps.set_testdf[9, ], aes(x = log(lambda), y = percent_diff), color = "orange", size = 5, shape = 8, alpha = 0.6) +
  
  scale_y_continuous(breaks = function(x) pretty(x, n = 10, min.n = 2)) + 
  theme_minimal() +
  labs(title = "MSE % Change vs. Log(λ) for sps.set Models: Training(Blue) vs. Testing(Orange)",
       x = "Log(Lambda)",
       y = "% Difference from Lambda.min MSE")

combinedPlot

# --- Problem 1 - b) - iv) Diagnostics of best model ----

#Plot diagnostics for best model (lambda min), and look for issues.

#Make sure we standardize residuals for diagnostic plots, as they are converted back to non standardized form by glmnet for outputs!

# Calculate residuals.
residuals <- y_testing - predict(lassoCV, newx = x_testing, s = lassoCV$lambda.min)

# Calculate estimated standard deviation of residuals.
residual_sd <- sd(residuals)

# Standardize residuals for diagnostic plots, as they are converted back to non standardized form by glmnet for outputs!
standardized_residuals <- residuals / residual_sd

# Fit a linear model using the standardized residuals for the diagnostic plots.
diagnostic_lm_model <- lm(standardized_residuals ~ predict(lassoCV, newx = x_testing, s = lassoCV$lambda.min))

# Visualize the diagnostic plots.
plot(diagnostic_lm_model)

##-------------------------------
### RIDGE Regression
##------------------------------

# --- Problem 1 - c) Replicate analysis with Ridge Regression ---

ridge_1 <- cv.glmnet(x_training, y_training, type.measure="mse", foldid = std_foldid, keep = TRUE, alpha =0)

plot(ridge_1, label = FALSE)
title(".....")

sps.set.ridge.df <- data.frame(lambda = ridge_1$lambda,nzero = ridge_1$nzero,mse = ridge_1$cvm) %>%
  filter(lambda >= ridge_1$lambda.min & lambda <= ridge_1$lambda.1se) %>%
  arrange(mse)

#Set our sps.set model lambda's to a numeric vector 
sps.set.ridge <- sps.set.ridge.df$lambda

#Performing predictions
prd.sps.set_ridge <- predict(ridge_1,newx=x_testing,s=sps.set.ridge)

cvxr <- glmnet(x_testing,y_testing,lambda=ridge_1$lambda, alpha=0)
plot(cvxr)

#Calculate MSEs
mse <- function (i,y) {
  mean ( (y-i)^2 ) }	

#Apply the function to the prediction matrix and the test set response variables
mse.sps.set.ridge <- apply(prd.sps.set_ridge, 2, mse,y_testing)

test_mse_ridge <- as.data.frame(mse.sps.set.ridge) %>%
  mutate(lambda = sps.set.ridge)

#Plotting mse across lambdas chosen
plot(log(sps.set.ridge),mse.sps.set.ridge)

#And percentage difference relative to best model
plot(log(sps.set.ridge),mse.sps.set.ridge/min(mse.sps.set.ridge) *100 -100)

#Plot diagnostics for best model (lambda min), and look for issues.

#Make sure we standardize residuals for diagnostic plots, as they are converted back to non standardized form by glmnet for outputs!

# Calculate residuals.
residuals <- y_testing - predict(ridge_1, newx = x_testing, s = ridge_1$lambda.min)

# Calculate estimated standard deviation of residuals.
residual_sd <- sd(residuals)

# Standardize residuals for diagnostic plots, as they are converted back to non standardized form by glmnet for outputs!
standardized_residuals <- residuals / residual_sd

# Fit a linear model using the standardized residuals for the diagnostic plots.
diagnostic_lm_model_ridge <- lm(standardized_residuals ~ predict(ridge_1, newx = x_testing, s = ridge_1$lambda.min))

# Visualize the diagnostic plots.
plot(diagnostic_lm_model_ridge)

#Here we will pick the second best mse, as covariates don't change in ridge and complexity is not changing here. 

# --- Problem 1 - d) Comparing models accross LASSO and Ridge ---

#Here we will pick the lse as the alternative model as well, as the higher bias lambda value = with smaller coefficients, will generzlize best and is still within the 1 std dev error - important provided the small dataset size.

#How do  best and alternative models from each compare?? Lower coefficients or less coefficients - better to keep them in? depends on biological issue at hand - and prior knowledge of the features. Based on MSE alone.


#Could plot lasso and ridge performance metrics. bar plot? 

#We need to consider how predictive performance would behave as new data is added as-well, if the model is being designed for application over time. 


##-------------------------------
### Net Elastic Regression
##------------------------------

# --- Problem 2 - a) i) Tune optimal alpha parameter

#For grid
alphas <- seq(.1, .9, .01)

#Create empty matrix for results
cvms_net <- matrix(NA,ncol=3,nrow=length(alphas))

mse.net <- NA
net.predict <- NA

for (i in 1:length(alphas)){
  
  alphas_val = alphas[i]
  
  cvfit <- cv.glmnet(x_training,y_training,foldid=std_foldid,alpha=alphas[i])
  
  indx <- which.min(cvfit$cvm)
  
  cvms_net[i,] <- c(cvfit$cvm[indx], cvfit$cvlo[indx],cvfit$cvup[indx])
  
  best_lambda = cvfit$lambda.min
  
  #predict for each model
  net.predict[i] <- predict(cvms_net,newx=x_testing,s=best_lambda)
  
  #compute mse
  mse.net[i] <- apply(net.predict,2,mse,y_testing)

}

#Name resulting columns
colnames(cvms_net) <- c("cvmin","cvlo","cvup")

par(mfrow=c(1,1))
#Plot MSE for all alpha values
plot(alphas,cvms_net[,1],pch=16,main="cvmin")

#Check best alpha from training set
alphas[which.min(cvms_net[,1])]

#And the testing set



#Now predict for each of the 81 models created with the minimum mse lambda values.




```

```{r}


# --- Problem 3 


#If you standardize coefficients you can simply rank the coefficients!! But when glm reports coefficients, it has converted them from standardized forms back to original units - thus to rank them, we need to scale them again!



```
