---
title: "Final_Assignment"
author: "Leon Edmiidz, Zach Ribau"
date: "2024-02-20"
output: pdf_document
---

```{r}


##----------------------
### Load Libraries & Data
##-----------------------

#Define function to install libraries if not already installed on system
install_if_needed <- function(package_name) {
  if (!require(package_name, character.only = TRUE)) {
    install.packages(package_name)
    library(package_name, character.only = TRUE)
  }
}

#Create vector with required libraries

packages <- c("tidyverse","glmnet", "ggplot2","lars", "caret", "ggplot2", "GGally", "knitr", "gridExtra")

#Loop over package vector and install and load libraries
for (pkg in packages) {
  install_if_needed(pkg)
}

#load in diabetes dataset
data("diabetes")
attach(diabetes)

##----------------------
### Data Splitting
##----------------------

# --- Problem 1 - a) ----

#

x_combined <- diabetes$x2

#Set seed for reproducibility
set.seed(1545)

#Hold out 42 observations for testing
test_set_size <- 42 
test.index <- sample.int(dim(x_combined)[1], test_set_size, replace = FALSE)

#Check for standardization, even though glmnet inherently standardizes
plot(as.ts(apply(x_combined, 2, sd)), xlab="Covariate", ylab="Standard Deviation") 

#Subset training data
x_training <- x_combined[-test.index, ]
y_training <- y[-test.index]

#Subset the testing data
x_testing <- x_combined[test.index, ]
y_testing <- y[test.index]


##-------------------##
### Cross Validation ###
##-------------------##

# --- Problem 1 - b) - i) ---- Performing cross validation with training data
lasso1 <- cv.glmnet(x_training, y_training, type.measure="mse", nfolds = 10, keep = TRUE, alpha =1)

#Svae the fold IDs to use for each method
std_foldid <- lasso1$foldid

plot(lasso1, label = FALSE)
title("LASSO Cross-Validation Plot")

# --- Problem 1 - b) - ii) Compare alternative models in sps.set ----

#Display a table containing alternative models, chosen as those with regularization temrs between lambda min and lambda 1se. Show the number of non zero coefficients as well.
sps.set <- data.frame(lambda = lasso1$lambda,nzero = lasso1$nzero,mse = lasso1$cvm) %>%
  filter(lambda >= lasso1$lambda.min & lambda <= lasso1$lambda.1se) %>%
  arrange(mse)

kable(sps.set)

#Plot the change in MSE specifically for our set of sps.set models, looking also at the number of nonzero coefficients (point size).

ggplot(sps.set, aes(x = log(lambda), y = mse)) +
  geom_point(aes(size = nzero)) + #Size based on nzero values
  geom_line() +
  geom_point(data = sps.set[2, ], aes(x = log(lambda), y = mse), color = "blue", size = 8, shape = 8, alpha = 0.75) + #Highight alternative model choice, index 2 from the dataframe.
  geom_text(aes(label = nzero), vjust = -0.8, hjust = 2, color = "darkgrey", size = 3.5) + #Label nzero values, moving them out of the way and colouring grey
  geom_text(data = sps.set[2, ], aes(x = log(lambda), y = mse, label = sprintf("lambda = %.6f", lambda)), vjust = -5, color = "blue", nudge_y = 0.1) + #move label down to avoid overlap w nzero labels
  scale_size_continuous(range = c(1, 6), guide = "none") + #Play around with size range for visibility of differences
  theme_minimal() +
  labs(title = "MSE vs.Lambda with # of Non-Zero Coefficients Labeled", x = "Log(Lambda)",y = "Mean Squared Error (mse)")

#Redefine sps.set as 

lambda <- sort(lasso1$lambda)
between <- which(lambda >= lasso1$lambda.min & lambda <= lasso1$lambda.1se)  
sps.set <- lambda[between] 

##-------------------------------
### Testing & Model Evaluations
##------------------------------

#Predict the response variables from the testing dataset
prd.sps.set <- predict(lasso1, newx = x_testing, s = sps.set)

#We should get predictions for each of the 9 models, across 42 observations.
dim(prd.sps.set)

#Now we will compute the MSE for all of our models by comparing to the ground truth response variables

#Defomomg the function shown in class
mse <- function (i,y) {
  mean ( (y-i)^2 ) }	

#Apply the function to the prediction matrix and the test set response variables
mse.sps.set <- apply(prd.sps.set,2,mse,y_testing)

test_mse_df <- as.data.frame(mse.sps.set) %>%
  mutate(lambda = sps.set)

#We can plot the fit for our best model as compared to the chosen alternative model
Lambda_min_fit <- data.frame(Actual = y_testing, Predicted = prd.sps.set[,1])

Lambda_min_plot <- ggplot(Lambda_min_fit , aes(x = Actual, y = Predicted)) +
  geom_point(color = "blue", size = 1) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "orange", size = 1,) +  
  labs(title = "Actual vs. Predicted Diabetes Progression Measures For the Best (Top) and Alternative (Bottom) models", x = "Actual", y = "Predicted") +
  theme_minimal()

Alternative_fit  <- data.frame(Actual = y_testing, Predicted = prd.sps.set[,2])

Alternative_plot <-  ggplot(Alternative_fit, aes(x = Actual, y = Predicted)) +
  geom_point(color = "blue", size = 1) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "orange", size = 1) +
  theme_minimal()

grid.arrange(Lambda_min_plot, Alternative_plot, ncol = 1)

#Plot 
plot(log(sps.set),mse.sps.set)

#Look at percentage difference from best model for each lambda value in sps.set
plot(log(sps.set),mse.sps.set/min(mse.sps.set) *100 -100)

# --- Problem 1 - b) - iii) Compare alternative models in sps.set ----

#Plot diagnostics for best model, and look for issues.

#Make sure we standardize residuals. 


##-------------------------------
### RIDGE Regression
##------------------------------

# --- Problem 1 - c)

ridge_1 <- cv.glmnet(x_training, y_training, type.measure="mse", foldid = std_foldid, keep = TRUE, alpha =0)

plot(ridge_1, label = FALSE)
title(".....")

sps.set.ridge <- data.frame(lambda = ridge_1$lambda,nzero = ridge_1$nzero,mse = ridge_1$cvm) %>%
  filter(lambda_ridge >= ridge_1$lambda.min & lambda_ridge <= ridge_1$lambda.1se) %>%
  arrange(mse)

kable(sps.set.ridge)

lambda <- sort(ridge_1$lambda)
between <- which(lambda >= ridge_1$lambda.min & lambda_ridge <= ridge_1$lambda.1se)  
sps.set.ridge <- lambda_ridge[between] 

#Performing predictions
prd.sps.set_ridge <- predict(ridge_1,newx=x_testing,s=sps.set.ridge)

cvxr <- glmnet(x_testing,y_testing,lambda=ridge_1$lambda, alpha=0)
plot(cvxr)

#Calculate MSEs
mse <- function (i,y) {
  mean ( (y-i)^2 ) }	

#Apply the function to the prediction matrix and the test set response variables
mse.sps.set.ridge <- apply(prd.sps.set_ridge ,2,mse,y_testing)

test_mse_ridge <- as.data.frame(mse.sps.set.ridge) %>%
  mutate(lambda = sps.set.ridge)

#Plotting mse across lambdas chosen
plot(log(sps.set.ridge),mse.sps.set.ridge)

#And percentage difference relative to best model
plot(log(sps.set.ridge),mse.sps.set.ridge/min(mse.sps.set.ridge) *100 -100)

#Here we will pick the second best mse, as covariates don't change in ridge and complexity is not changing here. 

# --- Problem 1 - d)

#Here we will pick the lse as the alternative model as well, as the higher bias lambda value = with smaller coefficients, will generzlize best and is still within the 1 std dev error - important provided the small dataset size.

#How do  best and alternative models from each compare?? Lower coefficients or less coeficients - better to keep them in? depends on biological issue at hand - and prior knowledge of the features. Based on MSE alone.



#Could plot lasso and ridge performance metrics. bar plot? 

#We need to consider how predictive performance would behave as new data is added as-well, if the model is being designed for application over time. 


# --- Problem 2 - a) i)

#For grid
alphas <- seq(.1, .9, .01)

#Create empty matrix for results
cvms_net <- matrix(NA,ncol=3,nrow=length(alphas))

for (i in 1:length(alphas)){
  
  cvfit <- cv.glmnet(x_training,y_training,foldid=std_foldid,alpha=alphas[i])
  indx <- which.min(cvfit$cvm)
  cvms_net[i,] <- c(cvfit$cvm[indx], cvfit$cvlo[indx],cvfit$cvup[indx])
}

colnames(cvms_net) <- c("cvmin","cvlo","cvup")

par(mfrow=c(1,1))
plot(alphas,cvms_net[,1],pch=16,main="cvmin")
plot(alphas,cvms_net[,2],pch=16,main="cvlo")
plot(alphas,cvms_net[,3],pch=16,main="cvup")

#Check best alpha for training
alphas[which.min(cvms_net[,1])]




```

```{r}


#If you standardize coefficients you can simply rank the coefficients!! But when glm reports coefficients, it has converted them from standardized forms back to original units - thus to rank them, we need to scale them again!


```


