---
title: "Final_Assignment"
author: "Leon Edmiidz, Zach Ribau"
date: "2024-02-20"
output: pdf_document
---

```{r}

##----------------------
### Load Libraries & Data
##-----------------------

#Define function to install libraries if not already installed on system
install_if_needed <- function(package_name) {
  if (!require(package_name, character.only = TRUE)) {
    install.packages(package_name)
    library(package_name, character.only = TRUE)
  }
}

#Create vector with required libraries

packages <- c("tidyverse","glmnet", "ggplot2","lars", "caret", "ggplot2", "GGally", "knitr", "gridExtra")

#Loop over package vector and install and load libraries
for (pkg in packages) {
  install_if_needed(pkg)
}

#load in diabetes dataset
data("diabetes")
attach(diabetes)

#Combine matrices to visualize as dataframe include row ID
full_dataset = data.frame(cbind(diabetes$x, diabetes$x2, y = diabetes$y))

##----------------------
### Data Splitting
##----------------------

# --- Problem 1 - a) ----

#Combine X and X2 matrices, as we will train a model with all available  features.

x_combined <- cbind(x, x2)

#Set seed for reproducibility
set.seed(1545)

#Hold out 42 observations for testing
test_set_size <- 42 
test.index <- sample.int(dim(x_combined)[1], test_set_size, replace = FALSE)

#Check for standardization, even though glmnet inherently standardizes
plot(as.ts(apply(x_combined, 2, sd)), xlab="Covariate", ylab="Standard Deviation") 

#Subset training data
x_training <- x_combined[-test.index, ]
y_training <- y[-test.index]

#Subset the testing data
x_testing <- x_combined[test.index, ]
y_testing <- y[test.index]

```


```{r}
##-------------------##
### Cross Validation ###
##-------------------##

# --- Problem 1 - b) - i) ---- Performing cross validation with training data


lasso1 <- cv.glmnet(x_training, y_training, type.measure="mse", nfolds=10)
plot(lasso1, label = FALSE)
title("LASSO Cross-Validation Plot")

# --- Problem 1 - b) - ii) Compare alternative models in sps.set ----

#Display a table containing alternative models, chosen as those with regularization temrs between lambda min and lambda 1se. Show the number of non zero coefficients as well.
sps.set <- data.frame(lambda = lasso1$lambda,
                      nzero = lasso1$nzero,
                      mse = lasso1$cvm) %>%
  filter(lambda >= lasso1$lambda.min & lambda <= lasso1$lambda.1se) %>%
  arrange(mse)

kable(sps.set)


#Plot the change in MSE specifically for our set of sps.set models, looking also at the number of nonzero coefficients (point size).

ggplot(sps.set, aes(x = log(lambda), y = mse)) +
  geom_point(aes(size = nzero)) + #Size based on nzero values
  geom_line() +
  geom_point(data = sps.set[2, ], aes(x = log(lambda), y = mse), color = "blue", size = 8, shape = 8, alpha = 0.75) + #Highight alternative model choice, index 2 from the dataframe.
  geom_text(aes(label = nzero), vjust = -0.8, hjust = 2, color = "darkgrey", size = 3.5) + #Label nzero values, moving them out of the way and colouring grey
  geom_text(data = sps.set[2, ], aes(x = log(lambda), y = mse, label = sprintf("lambda = %.6f", lambda)), vjust = -5, color = "blue", nudge_y = 0.1) + #move label down to avoid overlap w nzero labels
  scale_size_continuous(range = c(1, 6), guide = "none") + #Play around with size range for visibility of differences
  theme_minimal() +
  labs(title = "MSE vs.Lambda with # of Non-Zero Coefficients Labeled", x = "Log(Lambda)",y = "Mean Squared Error (mse)")


#Examining the evaluations_df, qe see two lambda values, 3.303388 and 3.625464 that are close to our best model and that have the same nzero value (18), thus we will consider the higher performing one, 3.303388. additional values share an nzero of 14, so we take only the 4.792651.

#Remove values corresponding to lower performing lambda values of the two pairs
sps.set <- sps.set[-c(3, 7), ]

prd.sps.set <- predict(lasso1, newx = x_testing, s = sps.set$lambda)


##-------------------------------
### Testing & Model Evaluations
##------------------------------

#Predict the response variables from the testing dataset
prd.sps.set <- predict(lasso1, newx = x_testing, s = sps.set$lambda)

#We should get predictions for each of the 9 models, across 42 observations.
dim(prd.sps.set)

#Now we will compute the MSE for all of our models by comparing to the ground truth response variables

#Defomomg the function shown in class
mse <- function (i,y) {
  mean ( (y-i)^2 ) }	

#Apply the function to the prediction matrix and the test set response variables
mse.sps.set <- apply(prd.sps.set,2,mse,y_testing)


#We can plot the fit for our best model as compared to the chosen alternative model
Lambda_min_fit <- data.frame(Actual = y_testing, Predicted = prd.sps.set[,1])

Lambda_min_plot <- ggplot(Lambda_min_fit , aes(x = Actual, y = Predicted)) +
  geom_point(color = "blue", size = 1) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "orange", size = 1,) +  
  labs(title = "Actual vs. Predicted Diabetes Progression Measures For the Best (Top) and Alternative (Bottom) models", x = "Actual", y = "Predicted") +
  theme_minimal()

Alternative_fit  <- data.frame(Actual = y_testing, Predicted = prd.sps.set[,2])

Alternative_plot <-  ggplot(Alternative_fit, aes(x = Actual, y = Predicted)) +
  geom_point(color = "blue", size = 1) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "orange", size = 1) +
  theme_minimal()

grid.arrange(Lambda_min_plot, Alternative_plot, ncol = 1)


```



#####
# Appproach for ranking covariates for PROBLEM-3 
"notes from lec" - hints
#####

```{r}


#If you standardize coefficients you can simply rank the coefficients!! But when glm reports coefficients, it has converted them from standardized forms back to original units - thus to rank them, we need to scale them again!


```


