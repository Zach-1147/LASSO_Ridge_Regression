---
title: "Final_Assignment"
author: "Leon Edmiidz, Zach Ribau"
date: "2024-02-20"
output: pdf_document
---

```{r}

##----------------------
### Load Libraries & Data
##-----------------------

#Define function to install libraries if not already installed on system
install_if_needed <- function(package_name) {
  if (!require(package_name, character.only = TRUE)) {
    install.packages(package_name)
    library(package_name, character.only = TRUE)
  }
}

#Create vector with required libraries

packages <- c("tidyverse","glmnet", "ggplot2","lars", "caret", "ggplot2", "GGally", "knitr", "gridExtra")

#Loop over package vector and install and load libraries
for (pkg in packages) {
  install_if_needed(pkg)
}

#load in diabetes dataset
data("diabetes")
attach(diabetes)

##----------------------
### Data Splitting
##----------------------

# --- Problem 1 - a) ----

x <- diabetes$x2

#Set seed for reproducibility, once at beginning of script
set.seed(1545)

#Hold out 42 observations for testing..)
test_set_size <- 42 
test.index <- sample.int(dim(x)[1], test_set_size, replace = FALSE)

#Check for standardization, even though glmnet inherently standardizes.
plot(as.ts(apply(x, 2, sd)), xlab="Covariate", ylab="Standard Deviation") 

#Subset training data
x_training <- x[-test.index, ]
y_training <- y[-test.index]

#Subset the testing data
x_testing <- x[test.index, ]
y_testing <- y[test.index]


##-------------------##
### Cross Validation ###
##-------------------##

# --- Problem 1 - b) - i) ---- Performing cross validation with training data

#Perform 10-fold cross-validation for LASSO model using MSE as measure, 10 folds, and setting keep as true to retain fold id's for consistency throughout the script.
lassoCV <- cv.glmnet(x_training, y_training, type.measure="mse", nfolds = 10, keep = TRUE, alpha =1)

#Save the fold IDs to a variable to set later
std_foldid <- lassoCV$foldid

#Plot the cross validation results, showing mean MSE versus lambda.
par(mar=c(5, 4, 6, 2))
plot(lassoCV)
title("LASSO Cross-Validation: MSE vs Lambda")

# --- Problem 1 - b) - ii) Compare alternative models in sps.set ----

#Display a table containing alternative models, chosen as those with regularization terms between lambda min and lambda 1se. Show the number of non zero coefficients as well. It will be saved as a dataframe for manual investigation

min_mse <- min(lassoCV$cvm) #Save min mse for formula used below

sps.set_df <- data.frame(lambda = lassoCV$lambda,nzero = lassoCV$nzero,mse = lassoCV$cvm) %>%
  filter(lambda >= lassoCV$lambda.min & lambda <= lassoCV$lambda.1se) %>%
  arrange(mse) %>%
mutate(percent_diff = ((mse - min_mse) /min_mse) * 100) #Add column to show percentage difference for each MSE from the best model (lambda.min)

#Print table to screen
kable(sps.set_df)

#Plot the change in mean MSE as a percentage of the minimum MSE specifically for our set of sps.set_df models, looking also at the number of nonzero coefficients (set to point size).

ggplot(sps.set_df, aes(x = log(lambda), y = percent_diff)) +
  geom_point(aes(size = nzero), alpha = 0.75) + 
  geom_line() +
  geom_point(data = sps.set_df[2, ], color = "blue", size = 8, shape = 8) + #Highlight alternative model choice
  geom_text(aes(label = nzero), vjust = -0.8, hjust = 2, color = "darkgrey", size = 3.5) +
  geom_text(data = sps.set_df[2, ], aes(label = sprintf("lambda = %.6f", lambda)), vjust = -5, color = "blue", nudge_y = 0.1) +
  scale_size_continuous(range = c(1, 6), guide = "none") + #Adjust point sizes based on nzero
  theme_minimal() +
  labs(title = "MSE % Change vs. Log(Î») for LASSO CV", # Corrected title for clarity and removed trailing comma
       x = "Log(Lambda)",
       y = "% Difference from Lambda.min MSE")


#Set our sps.set model lambda's to a numeric vector 

sps.set <- sps.set_df$lambda

##-------------------------------
### Testing & Model Evaluations
##------------------------------

#Predict the response variables from the testing dataset
prd.sps.set <- predict(lassoCV, newx = x_testing, s = sps.set)

#We should get predictions for each of the 9 models, across 42 observations.
dim(prd.sps.set)

#Now we will compute the MSE for all of our models by comparing to the ground truth disease progression values

#Defining the function shown in class
mse <- function (i,y) {
  mean ( (y-i)^2 ) }	

#Apply the function to the prediction matrix and the test set response variables
mse.sps.set <- apply(prd.sps.set,2,mse,y_testing)

min_mse_test <- min(mse.sps.set) #Save min mse for formula below

#Store summary stats in a data frame again to investigate
sps.set_test_df <- data.frame(
  lambda = sps.set,
  mse = mse.sps.set,
  percent_diff = ((mse.sps.set - min_mse_test) / min_mse_test) * 100)




#Plot 
plot(log(sps.set),mse.sps.set)

#Look at percentage difference from best model for each lambda value in sps.set
plot(log(sps.set),mse.sps.set/min(mse.sps.set) *100 -100)

# --- Problem 1 - b) - iii) Compare alternative models in sps.set ----

#Plot diagnostics for best model, and look for issues.

#Make sure we standardize residuals. 





##-------------------------------
### RIDGE Regression
##------------------------------

# --- Problem 1 - c)

ridge_1 <- cv.glmnet(x_training, y_training, type.measure="mse", foldid = std_foldid, keep = TRUE, alpha =0)

plot(ridge_1, label = FALSE)
title(".....")

sps.set.ridge <- data.frame(lambda = ridge_1$lambda,nzero = ridge_1$nzero,mse = ridge_1$cvm) %>%
  filter(lambda_ridge >= ridge_1$lambda.min & lambda_ridge <= ridge_1$lambda.1se) %>%
  arrange(mse)

kable(sps.set.ridge)

lambda <- sort(ridge_1$lambda)
between <- which(lambda >= ridge_1$lambda.min & lambda_ridge <= ridge_1$lambda.1se)  
sps.set.ridge <- lambda_ridge[between] 

#Performing predictions
prd.sps.set_ridge <- predict(ridge_1,newx=x_testing,s=sps.set.ridge)

cvxr <- glmnet(x_testing,y_testing,lambda=ridge_1$lambda, alpha=0)
plot(cvxr)

#Calculate MSEs
mse <- function (i,y) {
  mean ( (y-i)^2 ) }	

#Apply the function to the prediction matrix and the test set response variables
mse.sps.set.ridge <- apply(prd.sps.set_ridge ,2,mse,y_testing)

test_mse_ridge <- as.data.frame(mse.sps.set.ridge) %>%
  mutate(lambda = sps.set.ridge)

#Plotting mse across lambdas chosen
plot(log(sps.set.ridge),mse.sps.set.ridge)

#And percentage difference relative to best model
plot(log(sps.set.ridge),mse.sps.set.ridge/min(mse.sps.set.ridge) *100 -100)

#Here we will pick the second best mse, as covariates don't change in ridge and complexity is not changing here. 

# --- Problem 1 - d)

#Here we will pick the lse as the alternative model as well, as the higher bias lambda value = with smaller coefficients, will generzlize best and is still within the 1 std dev error - important provided the small dataset size.

#How do  best and alternative models from each compare?? Lower coefficients or less coeficients - better to keep them in? depends on biological issue at hand - and prior knowledge of the features. Based on MSE alone.


#Could plot lasso and ridge performance metrics. bar plot? 

#We need to consider how predictive performance would behave as new data is added as-well, if the model is being designed for application over time. 


# --- Problem 2 - a) i)

#For grid
alphas <- seq(.1, .9, .01)

#Create empty matrix for results
cvms_net <- matrix(NA,ncol=3,nrow=length(alphas))

mse.net <- NA
net.predict <- NA

for (i in 1:length(alphas)){
  
  alphas_val = alphas[i]
  
  cvfit <- cv.glmnet(x_training,y_training,foldid=std_foldid,alpha=alphas[i])
  
  indx <- which.min(cvfit$cvm)
  
  cvms_net[i,] <- c(cvfit$cvm[indx], cvfit$cvlo[indx],cvfit$cvup[indx])
  
  best_lambda = cvfit$lambda.min
  
  #predict for each model
  net.predict[i] <- predict(cvms_net,newx=x_testing,s=best_lambda)
  
  #compute mse
  mse.net[i] <- apply(net.predict,2,mse,y_testing)

}


#Name resulting columns
colnames(cvms_net) <- c("cvmin","cvlo","cvup")

par(mfrow=c(1,1))
#Plot MSE for all alpha values
plot(alphas,cvms_net[,1],pch=16,main="cvmin")

#Check best alpha from training set
alphas[which.min(cvms_net[,1])]

#And the testing set



#Now predict for each of the 81 models created with the minimum mse lambda values.




```

```{r}


#If you standardize coefficients you can simply rank the coefficients!! But when glm reports coefficients, it has converted them from standardized forms back to original units - thus to rank them, we need to scale them again!


```


