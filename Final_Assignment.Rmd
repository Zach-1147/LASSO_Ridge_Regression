---
title: "Final_Assignment"
author: "Leon Edmiidz, Zach Ribau"
date: "2024-02-20"
output: pdf_document
---

```{r}

##----------------------
### Load Libraries & Data
##-----------------------

#Define function to install libraries if not already installed on system
install_if_needed <- function(package_name) {
  if (!require(package_name, character.only = TRUE)) {
    install.packages(package_name)
    library(package_name, character.only = TRUE)
  }
}

#Create vector with required libraries

packages <- c("tidyverse","glmnet", "ggplot2","lars", "caret", "ggplot2", "GGally", "knitr", "gridExtra")

#Loop over package vector and install and load libraries
for (pkg in packages) {
  install_if_needed(pkg)
}

#load in diabetes dataset
data("diabetes")
attach(diabetes)

##----------------------
### Data Splitting
##----------------------

# --- Problem 1 - a) ----

x <- diabetes$x2

#Set seed for reproducibility, once at beginning of script
set.seed(1545)

#Hold out 42 observations for testing..)
test_set_size <- 42 
test.index <- sample.int(dim(x)[1], test_set_size, replace = FALSE)


#Check for standardization, even though glmnet inherently standardizes.
plot(as.ts(apply(x, 2, sd)), xlab="Covariate", ylab="Standard Deviation") 

#Subset training data
x_training <- x[-test.index, ]
y_training <- y[-test.index]

#Subset the testing data
x_testing <- x[test.index, ]
y_testing <- y[test.index]


##-------------------##
### Cross Validation ###
##-------------------##

# --- Problem 1 - b) - i) ---- Performing cross validation with training data

#Perform 10-fold cross-validation for LASSO model using MSE as measure, 10 folds, and setting keep as true to retain fold id's for consistency throughout the script.
lassoCV <- cv.glmnet(x_training, y_training, type.measure="mse", nfolds = 10, keep = TRUE, alpha =1)

#Save the fold IDs to a variable to set later
std_foldid <- lassoCV$foldid

#Plot the cross validation results, showing mean MSE versus lambda.
par(mar=c(5, 4, 6, 2))
plot(lassoCV)
title("LASSO Cross-Validation: MSE vs Lambda")

# --- Problem 1 - b) - ii) Compare alternative models in sps.set ----

#Display a table containing alternative models, chosen as those with regularization terms between lambda min and lambda 1se. Show the number of non zero coefficients as well. It will be saved as a dataframe for manual investigation

min_mse <- min(lassoCV$cvm) #Save min mse for formula used below

sps.set_df <- data.frame(lambda = lassoCV$lambda,nzero = lassoCV$nzero,mse = lassoCV$cvm) %>%
  filter(lambda >= lassoCV$lambda.min & lambda <= lassoCV$lambda.1se) %>%
  arrange(mse) %>%
  mutate(percent_diff = ((mse - min_mse) /min_mse) * 100) #Add column to show percentage difference for each MSE from the best model (lambda.min)

#Set our sps.set model lambda's to a numeric vector 
sps.set <- sps.set_df$lambda

##-------------------------------
### Testing & Model Evaluations
##------------------------------

# --- Problem 1 - b) - iii) Predict response variable and evaluate models  ----

#Predict the response variables from the testing dataset
prd.sps.set <- predict(lassoCV, newx = x_testing, s = sps.set)

#We should get predictions for each of the 9 models, across 42 observations.
dim(prd.sps.set)

#Now we will compute the MSE for all of our models by comparing to the ground truth disease progression values

#Defining the function shown in class
mse <- function (i,y) {
  mean ( (y-i)^2 ) }	

#Apply the function to the prediction matrix and the test set response variables
mse.sps.set <- apply(prd.sps.set,2,mse,y_testing)

min_mse_test <- min(mse.sps.set) #Save min mse for formula below

#Store summary stats in a data frame again to investigate
sps.set_testdf <- data.frame(
  lambda = sps.set,
  mse = mse.sps.set,
  nzero = sps.set_df$nzero,
  percent_diff = ((mse.sps.set - min_mse_test) / min_mse_test) * 100)


#We can combine training and testing % change MSE plots to look at the behaviour of the model error in testing and training compared to the best model. 

#Plot the change in mean MSE as a percentage of the minimum MSE specifically for our set of sps.set_df models, looking also at the number of nonzero coefficients (set to point size).

combinedPlot <- ggplot() +
  #Training data - Blue
  geom_line(data = sps.set_df, aes(x = log(lambda), y = percent_diff, group = 1), color = "blue") +
  geom_point(data = sps.set_df, aes(x = log(lambda), y = percent_diff), alpha = 0.75, color = "blue", size = 2) +
  
  geom_point(data = sps.set_df[9, ], aes(x = log(lambda), y = percent_diff), color = "black", size = 7, shape = 8) +
  
  geom_point(data = sps.set_df[1, ], aes(x = log(lambda), y = percent_diff), color = "red", size = 6, shape = 6) +
  
  #Testing data - Orange
  geom_line(data = sps.set_testdf, aes(x = log(lambda), y = percent_diff, group = 1), color = "orange") +
  geom_point(data = sps.set_testdf, aes(x = log(lambda), y = percent_diff), alpha = 0.75, color = "orange", size = 2) +
  geom_point(data = sps.set_testdf[9, ], aes(x = log(lambda), y = percent_diff), color = "black", size = 7, shape = 8) +
  
  geom_text(data = sps.set_df[9, ], aes(x = log(lambda), y = percent_diff, label = sprintf("lambda = %.6f", lambda)), vjust = -0.5, color = "black") +
  
  geom_text(data = sps.set_df[1, ], aes(x = log(lambda), y = percent_diff, label = sprintf("lambda = %.6f", lambda)), vjust = -4, hjust = 0, color = "red") +
  
  
  scale_y_continuous(breaks = function(x) pretty(x, n = 10, min.n = 2)) + 
  theme_minimal() +
  labs(title = "MSE % Change vs. Log(λ) for sps.set Models: Training(Blue) vs. Testing(Orange)",
       x = "Log(Lambda)",
       y = "% Difference from Lambda.min MSE")+
 theme(plot.title = element_text(size = 16))  # Adjust the size as needed

combinedPlot


# --- Problem 1 - b) - iv) Diagnostics of best model ----

# Calculate residuals.
residuals <- y_testing - predict(lassoCV, newx = x_testing, s = lassoCV$lambda.min)

# Calculate estimated standard deviation of residuals.
residual_sd <- sd(residuals)

# Standardize residuals for diagnostic plots, as they are converted back to non standardized form by glmnet for outputs!
standardized_residuals <- residuals / residual_sd

# Fit a linear model using the standardized residuals for the diagnostic plots.
diagnostic_lm_model <- lm(standardized_residuals ~ predict(lassoCV, newx = x_testing, s = lassoCV$lambda.min))

# Visualize the diagnostic plots.
plot(diagnostic_lm_model)

#Plot diagnostics for best model (lambda min), and look for issues.

# Calculate residuals.
residuals <- y_testing - predict(lassoCV, newx = x_testing, s = lassoCV$lambda.min)

# Calculate estimated standard deviation of residuals.
residual_sd <- sd(residuals)

# Standardize residuals for diagnostic plots, as they are converted back to non standardized form by glmnet for outputs!
standardized_residuals <- residuals / residual_sd

# Fit a linear model using the standardized residuals for the diagnostic plots.
diagnostic_lm_model <- lm(standardized_residuals ~ predict(lassoCV, newx = x_testing, s = lassoCV$lambda.min))

par(mfrow = c(2, 2))

# plot the diagnostics in a single frame
plot(diagnostic_lm_model)
par(mfrow = c(1, 1))



##-------------------------------
### RIDGE Regression
##------------------------------

# --- Problem 1 - c) Replicate analysis with Ridge Regression ---

ridge_1 <- cv.glmnet(x_training, y_training, type.measure="mse", foldid = std_foldid, keep = TRUE, alpha =0)

par(mar=c(5, 4, 6, 2))
plot(ridge_1, label = FALSE)
title("Ridge Cross-Validation: MSE vs Lambda")

sps.set.ridge.df <- data.frame(lambda = ridge_1$lambda,nzero = ridge_1$nzero,mse = ridge_1$cvm) %>%
  filter(lambda >= ridge_1$lambda.min & lambda <= ridge_1$lambda.1se) %>%
  arrange(mse)

#Set our sps.set model lambda's to a numeric vector 
sps.set.ridge <- sps.set.ridge.df$lambda

#Performing predictions
prd.sps.set_ridge <- predict(ridge_1,newx=x_testing,s=sps.set.ridge)

#Calculate MSEs
mse <- function (i,y) {
  mean ( (y-i)^2 ) }	

#Apply the function to the prediction matrix and the test set response variables
mse.sps.set.ridge <- apply(prd.sps.set_ridge, 2, mse,y_testing)

test_mse_ridge <- as.data.frame(mse.sps.set.ridge) %>%
  mutate(lambda = sps.set.ridge)

sps.set.ridge.df <- sps.set.ridge.df %>%
  mutate(test_mse = mse.sps.set.ridge)

sps.set.ridge.df <- sps.set.ridge.df %>%
  mutate(percent_diff_test = ((test_mse - min(test_mse)) / min(test_mse)) * 100)%>%
  mutate(percent_diff_train = ((mse - min(mse)) / min(mse)) * 100)


#Create model comparison plot, as before, for ridge
combinedPlot_ridge <- ggplot() +
  # Training data - Blue
  geom_line(data = sps.set.ridge.df, aes(x = log(lambda), y = percent_diff_train, group = 1), color = "blue") +
  geom_point(data = sps.set.ridge.df, aes(x = log(lambda), y = percent_diff_train), alpha = 0.75, color = "blue", size = 2) +
  
  # Highlighting specific points for training
  geom_point(data = sps.set.ridge.df[15, ], aes(x = log(lambda), y = percent_diff_train), color = "black", size = 7, shape = 8) +
  geom_point(data = sps.set.ridge.df[1, ], aes(x = log(lambda), y = percent_diff_train), color = "red", size = 6, shape = 6) +
  
  # Testing data - Orange
  geom_line(data = sps.set.ridge.df, aes(x = log(lambda), y = percent_diff_test, group = 1), color = "orange") +
  geom_point(data = sps.set.ridge.df, aes(x = log(lambda), y = percent_diff_test), alpha = 0.75, color = "orange", size = 2) +
  
  # Highlighting specific points for testing
  geom_point(data = sps.set.ridge.df[15, ], aes(x = log(lambda), y = percent_diff_test), color = "black", size = 7, shape = 8) +
  
  # Labels for highlighted points, adjusted for percent_diff
  geom_text(data = sps.set.ridge.df[15, ], aes(x = log(lambda), y = percent_diff_test, label = sprintf("lambda = %.6f", lambda)), vjust = -0.5,hjust = 1, color = "black") +
  geom_text(data = sps.set.ridge.df[1, ], aes(x = log(lambda), y = percent_diff_train, label = sprintf("lambda = %.6f", lambda)), vjust = -4, hjust = 0, color = "red") +
  
  scale_y_continuous(breaks = function(x) pretty(x, n = 10, min.n = 2)) + 
  theme_minimal() +
  labs(title = "Percent Difference vs. Log(λ) for Ridge Regression Models: Training(Blue) vs. Testing(Orange)",
       x = "Log(Lambda)",
       y = "Percent Difference from Min MSE")

combinedPlot_ridge

#Plot diagnostics for best model (lambda min), and look for issues.

# Calculate residuals.
residuals <- y_testing - predict(ridge_1, newx = x_testing, s = ridge_1$lambda.min)

# Calculate estimated standard deviation of residuals.
residual_sd <- sd(residuals)

# Standardize residuals for diagnostic plots, as they are converted back to non standardized form by glmnet for outputs!
standardized_residuals <- residuals / residual_sd

# Fit a linear model using the standardized residuals for the diagnostic plots.
diagnostic_lm_model_ridge <- lm(standardized_residuals ~ predict(ridge_1, newx = x_testing, s = ridge_1$lambda.min))

par(mfrow = c(2, 2))

#plot the diagnostics in a single frame
plot(diagnostic_lm_model_ridge)
par(mfrow = c(1, 1))



# --- Problem 1 - d) Comparing models across LASSO and Ridge ---



##-------------------------------
### Net Elastic Regression
##------------------------------

# --- Problem 2 - a) i) AND ii)  Tune optimal alpha parameter 

alphas <- seq(0.1, 0.9, by = 0.01)

#Create master dataframe to store various results of CV training and predictions accross the range of alpha values.
net_elastic <- data.frame(alpha = numeric(), train_mse = numeric(), test_mse = numeric())

models <- list(NA)

#Loop over alpha values
for (i in 1:length(alphas)){
  
  #Perform the cross validation for current alpha
  cvfit <- cv.glmnet(x_training, y_training, foldid = std_foldid, alpha = alphas[i])
  
  #Store model objects to list
  models[[i]] <- cvfit
  
  #Calculate the training MSE for the current model
  train_mse <- min(cvfit$cvm)  # Minimum cross-validation MSE
  
  lambda_min <- cvfit$lambda.min
  
  #Predict on the testing set using lambda.min for the current alpha
  prediction <- predict(cvfit, newx = x_testing, s = lambda_min)
  
  #Calculate the testing MSE for the current model
  test_mse <- mse(prediction, y_testing)
  
  #Append alpha, training MSE, and testing MSE values to the net_elastic data frame
  net_elastic <- rbind(net_elastic, data.frame(alpha = alphas[i], train_mse = train_mse, test_mse = test_mse, lambda_min = lambda_min))
}

#Extract best alpha from testing MSE measure
best_alpha_testing <- net_elastic$alpha[which.min(net_elastic$test_mse)]

#Index from alphas
best_alpha_index <- which(alphas == best_alpha_testing)

#Extract best performing model
best_net_model <- models[[best_alpha_index]]

#Plot the behaviour of the training cross validation error across the alpha values from the grid search
ggplot(net_elastic, aes(x = alpha, y = train_mse)) +
  geom_line() + geom_point() +
  labs(title = "Training CV-Error Across Different Alphas",
       x = "Alpha",
       y = "CV-Error") +
  theme_minimal()


best_alpha_training <- net_elastic$alpha[which.min(net_elastic$train_mse)]

# Plot the behavior of the testing MSE across the alpha values from the grid search
# Highlighting the best model from training data in red and the best model from testing data in blue
ggplot(net_elastic, aes(x = alpha, y = test_mse)) +
  geom_line() +
  geom_point() +
  geom_point(data = subset(net_elastic, alpha == best_alpha_training), 
             aes(x = alpha, y = test_mse), color = 'red', size = 4, alpha = 0.65) +
  geom_point(data = subset(net_elastic, alpha == best_alpha_testing), 
             aes(x = alpha, y = test_mse), color = 'blue', size = 4, alpha = 0.65) +
  labs(title = "Testing MSE Across Different Alphas",
       subtitle = paste("Best training alpha in red:", best_alpha_training, "\nBest testing alpha in blue:", best_alpha_testing),
       x = "Alpha",
       y = "Testing MSE") +
  theme_minimal()

# --- Problem 2 - b)  Diagnostic plots for best elastic net regression model

# Calculate residuals
residuals <- y_testing - predict(best_net_model, newx = x_testing, s = best_net_model$lambda.min)

# Calculate estimated standard deviation of residuals.
residual_sd <- sd(residuals)

# Standardize residuals for diagnostic plots, as they are converted back to non standardized form by glmnet for outputs!
standardized_residuals <- residuals / residual_sd

# Fit a linear model using the standardized residuals for the diagnostic plots.
diagnostic_lm_model_net <- lm(standardized_residuals ~ predict(best_net_model, newx = x_testing, s = best_net_model$lambda.min))

par(mfrow = c(2, 2))

#plot the diagnostics in a single frame
plot(diagnostic_lm_model_net)
par(mfrow = c(1, 1))

```

```{r}


# --- Problem 3 


```
