install.packages("lars")
library(lars)
#Define function to install libraries if not already installed on system
install_if_needed <- function(package_name) {
if (!require(package_name, character.only = TRUE)) {
install.packages(package_name)
library(package_name, character.only = TRUE)
}
}
#Create vector with required libraries
packages <- c("tidyverse", "ggplot2","lars")
#Loop over package vector and install and load libraries
for (pkg in packages) {
install_if_needed(pkg)
}
#Define function to install libraries if not already installed on system
install_if_needed <- function(package_name) {
if (!require(package_name, character.only = TRUE)) {
install.packages(package_name)
library(package_name, character.only = TRUE)
}
}
#Create vector with required libraries
packages <- c("tidyverse", "ggplot2","lars")
#Loop over package vector and install and load libraries
for (pkg in packages) {
install_if_needed(pkg)
library(pkg)
}
data(diabetes) # Load the dataset
View(diabetes)
View(diabetes)
#load in diabetes dataset
data(diabetes) # Load the dataset
#Define function to install libraries if not already installed on system
install_if_needed <- function(package_name) {
if (!require(package_name, character.only = TRUE)) {
install.packages(package_name)
library(package_name, character.only = TRUE)
}
}
#Create vector with required libraries
packages <- c("tidyverse", "ggplot2","lars")
#Loop over package vector and install and load libraries
for (pkg in packages) {
install_if_needed(pkg)
}
#load in diabetes dataset
data(diabetes) # Load the dataset
View(diabetes)
?diabetes
data(diabetes)
attach(diabetes)
View(diabetes)
str(diabetes)
names(diabetes)
attributes(diabetes)
dim(diabetes$y)
is.vector(diabetes$y)
length(diabetes$y)
dim(diabetes$x)
is.matrix(diabetes$x)
colnames(diabetes$x)
dim(diabetes$x2)
colnames(diabetes$x2)
?cv.glmnet
library(glmnet)
install.packages(glmnet)
install.packages(caret)
?cv.glmnet
apply(x,2,mean)  ## compute the mean of all the 10 covariates
apply(x,2,mean) < 1e-10
apply(x,2,var)  ## compute the variance of all the 10 covariates
apply(x,2,mean) < 1e-10
## The means are all zero! Numerical Error: If a < 1e-10 in R, we will treat
apply(x,2,mean)  ## compute the mean of all the 10 covariates
apply(x,2,mean) < 1e-10
apply(x,2,var)  ## compute the variance of all the 10 covariates
system.time(cvx <- cv.glmnet(x, y, type.measure="mse", nfolds=10))
library(glmnet)
library(glmnet)
install.packages("glmnet")
install.packages("caret")
library(glmnet)
library(caret)
?cv.glmnet
lambda <- cvx$lambda
set.seed(1212)
system.time(cvx <- cv.glmnet(x, y, type.measure="mse", nfolds=10))
attributes(cvx)
plot(cvx) ## what do you see?
lambda <- cvx$lambda
cvx$lambda.min
cvx$lambda.1se
is.vector(cvx$name)  ## check what is "name" about?
length(cvx$name)
cvx$name
cvx$name <- "CV-ERROR"  ## this is the name we used in the lecture
cvx$name   ## see the change
plot(cvx)  ## what's the label on the y-axes now
lambda[37] ; log(lambda[37])
coef(cvx,s=lambda[37])
coef(cvx,s=1.5)### Do we have lambda=1.5 in our lambda vector?
## If not, how do we get a model corresponding to it?
## see number of nonzero betas (model dimension)
coef(cvx,s = c(cvx$lambda.min,cvx$lambda.1se) )
?predict.cv.glmnet
predict(cvx,newx=x[1:2,],s=c(0.01, cvx$lambda.min,cvx$lambda.1se) )
prd.fl <- predict(cvx,newx=x,s=c(cvx$lambda.min,cvx$lambda.1se))
dim(prd.fl)
head(prd.fl)
mean((y - prd.fl[,1])^2)  ## against cvx$lambda.min
mean((y - prd.fl[,2])^2)  ## against cvx$lambda.1se
plot(cvx)  ## what's the label on the y-axes now
cvx$lambda
lambda <- sort(cvx$lambda)
lambda
wh <- which(lambda >= cvx$lambda.min & lambda <= cvx$lambda.1se)
## these are the indices of lambdas that we are looking for!
sps.set <- lambda[wh]  ## and this is the set we want!
sps.set
prd.sps.set <- predict(cvx,newx=x,s=sps.set )
## so we got the predictions, how many?
dim(prd.sps.set)
mse <- function (i,y) {
mean ( (y-i)^2 ) }		## quite simple!
## see if it works or not!
mean((y - prd.fl[,1])^2)  ## against cvx$lambda.min
mean((y - prd.fl[,2])^2)  ## against cvx$lambda.1se
apply(prd.fl , 2, mse, y)
## It does !!
mse.sps.set <- apply(prd.sps.set,2,mse,y)
str(mse.sps.set)
###
### Now compare these models for their predictive strength!
plot(sps.set,mse.sps.set)
plot(log(sps.set),mse.sps.set)	## better on the log-sale
plot(log(sps.set),mse.sps.set/min(mse.sps.set) *100 -100 )
csps.set <- coef(cvx,s=sps.set)
t(as.matrix(csps.set))[,-1]  ## intercept is always there!
cvxg <- glmnet(x,y,lambda=cvx$lambda)
?plot.glmnet
par(mfrow=c(1,2))
plot(cvxg) ## coefs against the norm - identify the models corresponding
## to L1-nomr=0 and its largest values!
plot(cvxg,xvar="lambda") ## coefs against the norm - again,
##identify the models corresponding to min/max lambda values.
##------********------********------********------********------********
install.packages("DAAG")
install.packages("DAAG")
(n <- nrow(diabetes))
train_indices <- sample(1:n, 400)
diabetes <- diabetes %>% mutate(row_id = row_number())
#Set seed prior to splitting 400 observations to a training set.
set.seed(1545)
n <- nrow(diabetes)
#Randomly select 400 observations from the dataset
training_set <- diabetes[sample(1:n, 400), ]
test_set <- diabetes %>%
filter(!row_id %in% training_set)
#Assign the testing as all observations not selected in the training_set
test_set <- diabetes %>%
filter(!row_id %in% training_set$row_id)
