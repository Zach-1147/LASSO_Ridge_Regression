install.packages("lars")
library(lars)
#Define function to install libraries if not already installed on system
install_if_needed <- function(package_name) {
if (!require(package_name, character.only = TRUE)) {
install.packages(package_name)
library(package_name, character.only = TRUE)
}
}
#Create vector with required libraries
packages <- c("tidyverse", "ggplot2","lars")
#Loop over package vector and install and load libraries
for (pkg in packages) {
install_if_needed(pkg)
}
#Define function to install libraries if not already installed on system
install_if_needed <- function(package_name) {
if (!require(package_name, character.only = TRUE)) {
install.packages(package_name)
library(package_name, character.only = TRUE)
}
}
#Create vector with required libraries
packages <- c("tidyverse", "ggplot2","lars")
#Loop over package vector and install and load libraries
for (pkg in packages) {
install_if_needed(pkg)
library(pkg)
}
data(diabetes) # Load the dataset
View(diabetes)
View(diabetes)
#load in diabetes dataset
data(diabetes) # Load the dataset
#Define function to install libraries if not already installed on system
install_if_needed <- function(package_name) {
if (!require(package_name, character.only = TRUE)) {
install.packages(package_name)
library(package_name, character.only = TRUE)
}
}
#Create vector with required libraries
packages <- c("tidyverse", "ggplot2","lars")
#Loop over package vector and install and load libraries
for (pkg in packages) {
install_if_needed(pkg)
}
#load in diabetes dataset
data(diabetes) # Load the dataset
View(diabetes)
?diabetes
data(diabetes)
attach(diabetes)
View(diabetes)
str(diabetes)
names(diabetes)
attributes(diabetes)
dim(diabetes$y)
is.vector(diabetes$y)
length(diabetes$y)
dim(diabetes$x)
is.matrix(diabetes$x)
colnames(diabetes$x)
dim(diabetes$x2)
colnames(diabetes$x2)
?cv.glmnet
library(glmnet)
install.packages(glmnet)
install.packages(caret)
?cv.glmnet
apply(x,2,mean)  ## compute the mean of all the 10 covariates
apply(x,2,mean) < 1e-10
apply(x,2,var)  ## compute the variance of all the 10 covariates
apply(x,2,mean) < 1e-10
## The means are all zero! Numerical Error: If a < 1e-10 in R, we will treat
apply(x,2,mean)  ## compute the mean of all the 10 covariates
apply(x,2,mean) < 1e-10
apply(x,2,var)  ## compute the variance of all the 10 covariates
system.time(cvx <- cv.glmnet(x, y, type.measure="mse", nfolds=10))
library(glmnet)
library(glmnet)
install.packages("glmnet")
install.packages("caret")
library(glmnet)
library(caret)
?cv.glmnet
lambda <- cvx$lambda
set.seed(1212)
system.time(cvx <- cv.glmnet(x, y, type.measure="mse", nfolds=10))
attributes(cvx)
plot(cvx) ## what do you see?
lambda <- cvx$lambda
cvx$lambda.min
cvx$lambda.1se
is.vector(cvx$name)  ## check what is "name" about?
length(cvx$name)
cvx$name
cvx$name <- "CV-ERROR"  ## this is the name we used in the lecture
cvx$name   ## see the change
plot(cvx)  ## what's the label on the y-axes now
lambda[37] ; log(lambda[37])
coef(cvx,s=lambda[37])
coef(cvx,s=1.5)### Do we have lambda=1.5 in our lambda vector?
## If not, how do we get a model corresponding to it?
## see number of nonzero betas (model dimension)
coef(cvx,s = c(cvx$lambda.min,cvx$lambda.1se) )
?predict.cv.glmnet
predict(cvx,newx=x[1:2,],s=c(0.01, cvx$lambda.min,cvx$lambda.1se) )
prd.fl <- predict(cvx,newx=x,s=c(cvx$lambda.min,cvx$lambda.1se))
dim(prd.fl)
head(prd.fl)
mean((y - prd.fl[,1])^2)  ## against cvx$lambda.min
mean((y - prd.fl[,2])^2)  ## against cvx$lambda.1se
plot(cvx)  ## what's the label on the y-axes now
cvx$lambda
lambda <- sort(cvx$lambda)
lambda
wh <- which(lambda >= cvx$lambda.min & lambda <= cvx$lambda.1se)
## these are the indices of lambdas that we are looking for!
sps.set <- lambda[wh]  ## and this is the set we want!
sps.set
prd.sps.set <- predict(cvx,newx=x,s=sps.set )
## so we got the predictions, how many?
dim(prd.sps.set)
mse <- function (i,y) {
mean ( (y-i)^2 ) }		## quite simple!
## see if it works or not!
mean((y - prd.fl[,1])^2)  ## against cvx$lambda.min
mean((y - prd.fl[,2])^2)  ## against cvx$lambda.1se
apply(prd.fl , 2, mse, y)
## It does !!
mse.sps.set <- apply(prd.sps.set,2,mse,y)
str(mse.sps.set)
###
### Now compare these models for their predictive strength!
plot(sps.set,mse.sps.set)
plot(log(sps.set),mse.sps.set)	## better on the log-sale
plot(log(sps.set),mse.sps.set/min(mse.sps.set) *100 -100 )
csps.set <- coef(cvx,s=sps.set)
t(as.matrix(csps.set))[,-1]  ## intercept is always there!
cvxg <- glmnet(x,y,lambda=cvx$lambda)
?plot.glmnet
par(mfrow=c(1,2))
plot(cvxg) ## coefs against the norm - identify the models corresponding
## to L1-nomr=0 and its largest values!
plot(cvxg,xvar="lambda") ## coefs against the norm - again,
##identify the models corresponding to min/max lambda values.
##------********------********------********------********------********
install.packages("DAAG")
install.packages("DAAG")
(n <- nrow(diabetes))
train_indices <- sample(1:n, 400)
diabetes <- diabetes %>% mutate(row_id = row_number())
#Set seed prior to splitting 400 observations to a training set.
set.seed(1545)
n <- nrow(diabetes)
#Randomly select 400 observations from the dataset
training_set <- diabetes[sample(1:n, 400), ]
test_set <- diabetes %>%
filter(!row_id %in% training_set)
#Assign the testing as all observations not selected in the training_set
test_set <- diabetes %>%
filter(!row_id %in% training_set$row_id)
#Look at out best model
lasso1$lambda.min
##----------------------
### Load Libraries & Data
##-----------------------
#Define function to install libraries if not already installed on system
install_if_needed <- function(package_name) {
if (!require(package_name, character.only = TRUE)) {
install.packages(package_name)
library(package_name, character.only = TRUE)
}
}
#Create vector with required libraries
packages <- c("tidyverse","glmnet", "ggplot2","lars", "caret", "ggplot2", "GGally")
#Loop over package vector and install and load libraries
for (pkg in packages) {
install_if_needed(pkg)
}
#load in diabetes dataset
data("diabetes")
attach(diabetes)
#Combine matrices to visualize as dataframe include row ID
full_dataset = data.frame(cbind(diabetes$x, diabetes$x2, y = diabetes$y))
##----------------------
### Data Splitting
##----------------------
#Combine X and X2 matrices, as we will train a model with all available  features.
x_combined <- cbind(x, x2)
#Set seed for reproducibility
set.seed(1545)
#Hold out 42 observations for testing
test_set_size <- 42
test.index <- sample.int(dim(x_combined)[1], test_set_size, replace = FALSE)
#Check for standardization, even though glmnet inherently standardizes
plot(as.ts(apply(x_combined, 2, sd)), xlab="Covariate", ylab="Standard Deviation")
#Subset training data
x_training <- x_combined[-test.index, ]
y_training <- y[-test.index]
#Subset the testing data
x_testing <- x_combined[test.index, ]
y_testing <- y[test.index]
lasso1$lambda.min
lasso1 <- cv.glmnet(x_training, y_training, type.measure="mse", nfolds=10)
lasso1$lambda.min
lasso1$lambda.1se
coefficients <- broom::tidy(lasso1, exponentiate = TRUE)
sps_set <- coefficients %>%
filter(lambda >= lambda_min & lambda <= lambda_1se) %>%
group_by(lambda) %>%
summarize(total_nonzero = sum(abs(estimate) > 0)) %>%
arrange(desc(lambda))
#And the conservative lse
lambda_lse <- lasso1$lambda.1se
#Look at the value corresponding to our best model,
lambda_min <- lasso1$lambda.min
sps_set <- coefficients %>%
filter(lambda >= lambda_min & lambda <= lambda_1se) %>%
group_by(lambda) %>%
summarize(total_nonzero = sum(abs(estimate) > 0)) %>%
arrange(desc(lambda))
#And the conservative lse
lambda_1se <- lasso1$lambda.1se
sps_set <- coefficients %>%
filter(lambda >= lambda_min & lambda <= lambda_1se) %>%
group_by(lambda) %>%
summarize(total_nonzero = sum(abs(estimate) > 0)) %>%
arrange(desc(lambda))
View(sps_set)
View(coefficients)
coefficients <- tidy(lasso1, exponentiate = TRUE)
sps_set <- coefficients %>%
filter(lambda >= lambda_min & lambda <= lambda_1se) %>%
group_by(lambda) %>%
summarize(total_nonzero = sum(abs(estimate) > 0)) %>%
arrange(desc(lambda))
View(sps_set)
sps_set <- coefficients %>%
filter(lambda >= lambda_min & lambda <= lambda_1se) %>%
group_by(lambda) %>%
summarize(total_nonzero = nzero) %>%
arrange(desc(lambda))
View(sps_set)
lambda_min <- lasso1$lambda.min
#Look at the value corresponding to our best model,
(lambda_min <- lasso1$lambda.min)
sps_set <- sps_set[-c(3, 7), ]
View(sps_set)
print(lambda_min)
coef(cvx,s=1.5)### Do we have lambda=1.5 in our lambda vector?
coef(cvx,s = c(cvx$lambda.min,cvx$lambda.1se) )
mean((y - prd.fl[,1])^2)  ## against cvx$lambda.min
mean((y - prd.fl[,2])^2)  ## against cvx$lambda.1se
lambda <- sort(cvx$lambda)
lambda
wh <- which(lambda >= cvx$lambda.min & lambda <= cvx$lambda.1se)
## these are the indices of lambdas that we are looking for!
wh
sps.set <- lambda[wh]  ## and this is the set we want!
sps.set
prd.sps.set <- predict(cvx,newx=x,s=sps.set )
dim(prd.sps.set)
prd.sps.set <- predict(sps_set, newx=x_testing,s=sps_set)
prd.sps.set <- predict(lasso1, newx=x_testing,s=sps_set$lambda)
dim(prd.sps.set)
mse <- function (i,y) {
mean ( (y-i)^2 ) }
