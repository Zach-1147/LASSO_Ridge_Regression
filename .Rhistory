setwd("C:/Users/zachr/Desktop/V-38/Workflow/Current Projects/BINF 6970 Assignment 2/Data & Code/6970_Assignment-2")
plot(lasso1$lambda, lasso1$cvm, type="b", log="x", xlab="Lambda", ylab="MSE")
##----------------------
### Load Libraries & Data
##-----------------------
#Define function to install libraries if not already installed on system
install_if_needed <- function(package_name) {
if (!require(package_name, character.only = TRUE)) {
install.packages(package_name)
library(package_name, character.only = TRUE)
}
}
#Create vector with required libraries
packages <- c("tidyverse","glmnet", "ggplot2","lars", "caret", "ggplot2", "GGally")
#Loop over package vector and install and load libraries
for (pkg in packages) {
install_if_needed(pkg)
}
#load in diabetes dataset
data("diabetes")
attach(diabetes)
#Combine matrices to visualize as dataframe include row ID
full_dataset = data.frame(cbind(diabetes$x, diabetes$x2, y = diabetes$y))
##----------------------
### Data Splitting
##----------------------
#Combine X and X2 matrices, as we will train a model with all available  features.
x_combined <- cbind(x, x2)
#Set seed for reproducibility
set.seed(1545)
#Hold out 42 observations for testing
test_set_size <- 42
test.index <- sample.int(dim(x_combined)[1], test_set_size, replace = FALSE)
#Check for standardization, even though glmnet inherently standardizes
plot(as.ts(apply(x_combined, 2, sd)), xlab="Covariate", ylab="Standard Deviation")
#Subset training data
x_training <- x_combined[-test.index, ]
y_training <- y[-test.index]
#Subset the testing data
x_testing <- x_combined[test.index, ]
y_testing <- y[test.index]
lasso1 <- cv.glmnet(x_training, y_training, type.measure="mse", nfolds=10)
plot(lasso1)
#Look at the value corresponding to our best model,
lambda_min <- lasso1$lambda.min
#And the conservative lse
lambda_1se <- lasso1$lambda.1se
#Now consider a set of models between these values
#We can convert our momdel coefficients accross lambda values to a dataframe
coefficients <- tidy(lasso1, exponentiate = TRUE)
packages <- c("tidyverse","glmnet","broom", "ggplot2","lars", "caret", "ggplot2", "GGally")
#Loop over package vector and install and load libraries
for (pkg in packages) {
install_if_needed(pkg)
}
coefficients <- tidy(lasso1, exponentiate = TRUE)
#List various lambda values within the range of lambda_min to lambda_1se, summarize with the number of non zero coefficients to assess complexity.
sps_set <- coefficients %>%
filter(lambda >= lambda_min & lambda <= lambda_1se) %>%
group_by(lambda) %>%
summarize(total_nonzero = nzero) %>%
arrange(desc(lambda))
plot(lasso1$lambda, lasso1$cvm, type="b", log="x", xlab="Lambda", ylab="MSE")
abline(v=lasso$lambda.min, col="red")
plot(lasso1$lambda, lasso1$cvm, type="b", log="x", xlab="Lambda", ylab="MSE")
abline(v=lasso$lambda.min, col="red")
plot(lasso1$lambda, lasso1$cvm, type="b", log="x", xlab="Lambda", ylab="MSE")
abline(v=lasso1$lambda.min, col="red")
abline(v=lasso1$lambda.1se, col="blue")
View(coefficients)
lambda_nzero_df <- data.frame(lambda = lasso1$lambda, nzero = non_zero_coefficients)
non_zero_coefficients <- lasso1$nzero
lambda_nzero_df <- data.frame(lambda = lasso1$lambda, nzero = non_zero_coefficients)
View(lambda_nzero_df)
non_zero_coefficients <- lasso1$nzero
mse_values <- lasso1$cvm
evaluation_df <- data.frame(lambda = lasso1$lambda, nzero = non_zero_coefficients)
evaluation_df <- data.frame(lambda = lasso1$lambda,
nzero = non_zero_coefficients,
mse = mse_values)
View(evaluation_df)
sps_set <- evaluation_df %>%
filter(lambda >= lambda_min & lambda <= lambda_1se) %>%
arrange(desc(lambda))
View(sps_set)
sps_set <- sps_set[-c(3, 7), ]
non_zero_coefficients <- lasso1$nzero
mse_values <- lasso1$cvm
evaluation_df <- data.frame(lambda = lasso1$lambda, nzero = non_zero_coefficients)
evaluation_df <- data.frame(lambda = lasso1$lambda,
nzero = non_zero_coefficients,
mse = mse_values)
#We can now filter this datagrame to only include our sps_set models, and comapre them accordingly.
#List various lambda values within the range of lambda_min to lambda_1se
sps_set <- evaluation_df %>%
filter(lambda >= lambda_min & lambda <= lambda_1se) %>%
arrange(desc(lambda))
# Rank the lambda values based on nzero and mse separately
evaluation_df <- evaluation_df %>%
mutate(rank_nzero = rank(nzero),  # Rank based on the number of non-zero coefficients
rank_mse = rank(mse),      # Rank based on MSE
# Combine ranks into a score, lower score is better
score = (rank_nzero + rank_mse) / 2) %>%
arrange(score)  # Arrange by score to see best lambda values first
View(evaluation_df)
#We can extract each lambda values number of non-zero coefficients, as well as mse values, and save to a dataframe
non_zero_coefficients <- lasso1$nzero
mse_values <- lasso1$cvm
evaluation_df <- data.frame(lambda = lasso1$lambda, nzero = non_zero_coefficients)
evaluation_df <- data.frame(lambda = lasso1$lambda,
nzero = non_zero_coefficients,
mse = mse_values)
#List various lambda values within the range of lambda_min to lambda_1se
sps_set <- evaluation_df %>%
filter(lambda >= lambda_min & lambda <= lambda_1se) %>%
arrange(desc(lambda))
View(sps_set)
ggplot(sps_set, aes(x = nzero, y = mse)) +
geom_point() +
geom_line() + # Optional, to connect points in order of lambda values
theme_minimal() +
labs(title = "Number of Non-Zero Coefficients vs. MSE",
x = "Number of Non-Zero Coefficients (nzero)",
y = "Mean Squared Error (mse)")
ggplot(sps_set, aes(x = nzero, y = mse)) +
geom_point() +  # Plot all points
geom_line() +   # Connect points with lines
geom_point(data = sps_set[6, ], aes(x = nzero, y = mse), color = "red", size = 4, shape = 8) + # Highlight index 6
theme_minimal() +
labs(title = "Number of Non-Zero Coefficients vs. MSE",
x = "Number of Non-Zero Coefficients (nzero)",
y = "Mean Squared Error (mse)")
coefficients_at_index_5 <- coef(lasso1, s = lambda_at_index_5)
lambda_at_index_5 <- sps_set$lambda[5]
coefficients_at_index_5 <- coef(lasso1, s = lambda_at_index_5)
View(coefficients_at_index_5)
coefficients_df <- as.data.frame(coefficients_at_index_5[coefficients_at_index_5 != 0, , drop = FALSE])
prd.sps.set <- predict(lasso1, newx=x_testing,s=sps_set$lambda)
View(sps_set)
#Predict response variable in testing data using our models in sps.set
prd.sps.set <- predict(lasso1, newx=x_testing,s=as.matrix(sps_set$lambda))
prd.sps.set <- predict(lasso1, newx = x_testing, s = sps_set$lambda)
library(glmnet)
prd.sps.set <- predict(lasso1, newx = x_testing, s = sps_set$lambda)
dim(prd.sps.set)
#Predict the response variables from the testing dataset
(prd.sps.set <- predict(lasso1, newx = x_testing, s = sps_set$lambda))
as.data.frame(prd.sps.set)
View(prd.sps.set)
mse <- function (i,y) {
mean ( (y-i)^2 ) }
#Apply the function to the prediction matrix and the test set response variables
mse.sps.set <- apply(prd.sps.set,2,mse,y_training)
rm(list=ls())
##----------------------
### Load Libraries & Data
##-----------------------
#Define function to install libraries if not already installed on system
install_if_needed <- function(package_name) {
if (!require(package_name, character.only = TRUE)) {
install.packages(package_name)
library(package_name, character.only = TRUE)
}
}
#Create vector with required libraries
packages <- c("tidyverse","glmnet", "ggplot2","lars", "caret", "ggplot2", "GGally")
#Loop over package vector and install and load libraries
for (pkg in packages) {
install_if_needed(pkg)
}
#load in diabetes dataset
data("diabetes")
attach(diabetes)
#Combine matrices to visualize as dataframe include row ID
full_dataset = data.frame(cbind(diabetes$x, diabetes$x2, y = diabetes$y))
##----------------------
### Data Splitting
##----------------------
#Combine X and X2 matrices, as we will train a model with all available  features.
x_combined <- cbind(x, x2)
#Set seed for reproducibility
set.seed(1545)
#Hold out 42 observations for testing
test_set_size <- 42
test.index <- sample.int(dim(x_combined)[1], test_set_size, replace = FALSE)
#Check for standardization, even though glmnet inherently standardizes
plot(as.ts(apply(x_combined, 2, sd)), xlab="Covariate", ylab="Standard Deviation")
#Subset training data
x_training <- x_combined[-test.index, ]
y_training <- y[-test.index]
#Subset the testing data
x_testing <- x_combined[test.index, ]
y_testing <- y[test.index]
##----------------------
### Cross Validation
##----------------------
lasso1 <- cv.glmnet(x_training, y_training, type.measure="mse", nfolds=10)
plot(lasso1)
#Look at the value corresponding to our best model,
lambda_min <- lasso1$lambda.min
#And the conservative lse
lambda_1se <- lasso1$lambda.1se
#Now consider a set of models between these values
#We can extract each lambda values number of non-zero coefficients, as well as mse values, and save to a dataframe
non_zero_coefficients <- lasso1$nzero
mse_values <- lasso1$cvm
evaluation_df <- data.frame(lambda = lasso1$lambda, nzero = non_zero_coefficients)
evaluation_df <- data.frame(lambda = lasso1$lambda,
nzero = non_zero_coefficients,
mse = mse_values)
#We can now filter this datagrame to only include our sps_set models, and comapre them accordingly.
#List various lambda values within the range of lambda_min to lambda_1se
sps_set <- evaluation_df %>%
filter(lambda >= lambda_min & lambda <= lambda_1se) %>%
arrange(desc(lambda))
ggplot(sps_set, aes(x = nzero, y = mse)) +
geom_point() +  # Plot all points
geom_line() +   # Connect points with lines
geom_point(data = sps_set[6, ], aes(x = nzero, y = mse), color = "red", size = 4, shape = 8) + # Highlight index 6
theme_minimal() +
labs(title = "Number of Non-Zero Coefficients vs. MSE",
x = "Number of Non-Zero Coefficients (nzero)",
y = "Mean Squared Error (mse)")
#Examining the evaluations_df, qe see two lambda values, 3.303388 and 3.625464 that are close to our best model and that have the same nzero value (18), thus we will consider the higher performing one, 3.303388. additional values share an nzero of 14, so we take only the 4.792651.
#Remove values corresponding to lower performing lambda values of the two pairs
sps_set <- sps_set[-c(3, 7), ]
prd.sps.set <- predict(lasso1, newx = x_testing, s = sps_set$lambda)
##-------------------------------
### Testing & Model Evaluations
##------------------------------
#Predict the response variables from the testing dataset
prd.sps.set <- predict(lasso1, newx = x_testing, s = sps_set$lambda)
#We should get predictions for each of the 9 models, across 42 observations.
dim(prd.sps.set)
#Now we will compute the MSE for all of our models by comparing to the ground truth response variables
#Defomomg the function shown in class
mse <- function (i,y) {
mean ( (y-i)^2 ) }
#Apply the function to the prediction matrix and the test set response variables
mse.sps.set <- apply(prd.sps.set,2,mse,y_training)
#Apply the function to the prediction matrix and the test set response variables
mse.sps.set <- apply(prd.sps.set,2,mse,y_testing)
#Apply the function to the prediction matrix and the test set response variables
mse.sps.set <- apply(prd.sps.set,2,mse,y_testing)
str(mse.sps.set)
plot(sps.set,mse.sps.set)
plot(sps_set,mse.sps.set)
plot(log(sps_set),mse.sps.set)	## better on the log-sale
## To see how much the difference really is!
plot(log(sps_set),mse.sps.set/min(mse.sps.set) *100 -100 )
## To see how much the difference really is!
plot(log(sps_set),mse.sps.set/min(mse.sps.set) *100 -100 )
ggplot(sps_set, aes(x = nzero, y = test_mse)) +
geom_point() +
geom_line(group = 1) +
theme_minimal() +
labs(title = "Predictive Performance vs. Model Complexity",
x = "Number of Non-Zero Coefficients",
y = "Mean Squared Error on Test Set")
ggplot(sps_set, aes(x = nzero, y = mse.sps.set)) +
geom_point() +
geom_line(group = 1) +
theme_minimal() +
labs(title = "Predictive Performance vs. Model Complexity",
x = "Number of Non-Zero Coefficients",
y = "Mean Squared Error on Test Set")
new_model_mse_df <- data.frame(lambda = sps_set$lambda, test_mse = mse.sps.set)
View(new_model_mse_df)
evaluation_df$lambda <- as.numeric(evaluation_df$lambda)
sps_set$lambda <- as.numeric(sps_set$lambda)
evaluation_df$lambda <- as.numeric(evaluation_df$lambda)
testing_mse$lambda <- as.numeric(testing_mse$lambda)
testing_mse <- data.frame(lambda = sps_set$lambda, test_mse = mse.sps.set)
testing_mse$lambda <- as.numeric(testing_mse$lambda)
testVStrain_mse <- inner_join(evaluation_df, sps_set, by = "lambda")
View(testVStrain_mse)
testVStrain_mse <- inner_join(evaluation_df, sps_set, by = "lambda") %>% select(testVStrain_mse, -nzero.y)
testVStrain_mse <- inner_join(evaluation_df, sps_set, by = "lambda") %>%
select(-nzero.y)
View(testVStrain_mse)
testVStrain_mse <- testVStrain_mse %>%
rename(mse.x = "mse_train", mse.y = "mse_test")
testVStrain_mse <- testVStrain_mse %>%
rename(mse_train = "mse.x", mse_test = "mse.y")
View(testVStrain_mse)
testVStrain_mse <- inner_join(evaluation_df, testing_mse, by = "lambda") %>%
select(-nzero.y)
testVStrain_mse <- inner_join(evaluation_df, testing_mse, by = "lambda")
View(testVStrain_mse)
testVStrain_mse <- testVStrain_mse %>%
mutate(Difference = mse_test - mse)  %>%
arrange(desc(Difference))
testVStrain_mse <- testVStrain_mse %>%
mutate(Difference = test_mse - mse)  %>%
arrange(desc(Difference))
View(testVStrain_mse)
testVStrain_mse <- testVStrain_mse %>%
mutate(Difference = test_mse - mse)  %>%
arrange(asc(Difference))
testVStrain_mse <- testVStrain_mse %>%
mutate(Difference = test_mse - mse)  %>%
arrange((Difference))
View(testVStrain_mse)
prd.sps.set$nzer0
prd.sps.set$nzero
as.datae.frame(prd.sps.set$nzero)
as.data.frame(prd.sps.set$nzero)
as.data.frame(prd.sps.set)
testVStrain_mse <- testVStrain_mse %>%
mutate(Difference = test_mse - mse)  %>%
rename(train_nzero = "nzero") )  %>%
testVStrain_mse <- testVStrain_mse %>%
mutate(Difference = test_mse - mse)  %>%
rename(train_nzero = "nzero")  %>%
arrange((Difference))
View(testVStrain_mse)
predictions$nzero
predictions <- as.data.frame(prd.sps.set)
predictions$nzero
View(predictions)
View(prd.sps.set)
prd.sps.set <- predict(lasso1, newx = x_testing, s = sps_set$lambda)
View(prd.sps.set)
plot(sps_set, mse.sps.set)
library(ggplot2)
ggplot(data = sps_set, aes(x = lambda, y = mse.sps.set)) +
geom_point() +
labs(x = "Lambda", y = "MSE", title = "MSE vs. Lambda")
ggplot(data = sps_set, aes(x = lambda, y = test_mse)) +
geom_point() +
geom_line() + # Optionally add a line to connect the points
labs(x = "Lambda", y = "MSE", title = "MSE vs. Lambda")
rm(list =ls())
##----------------------
### Load Libraries & Data
##-----------------------
#Define function to install libraries if not already installed on system
install_if_needed <- function(package_name) {
if (!require(package_name, character.only = TRUE)) {
install.packages(package_name)
library(package_name, character.only = TRUE)
}
}
#Create vector with required libraries
packages <- c("tidyverse","glmnet", "ggplot2","lars", "caret", "ggplot2", "GGally", "knitr", "gridExtra")
#Loop over package vector and install and load libraries
for (pkg in packages) {
install_if_needed(pkg)
}
#load in diabetes dataset
data("diabetes")
attach(diabetes)
##----------------------
### Data Splitting
##----------------------
# --- Problem 1 - a) ----
x <- diabetes$x2
#Set seed for reproducibility, once at beginning of script
set.seed(1545)
#Hold out 42 observations for testing..)
test_set_size <- 42
test.index <- sample.int(dim(x)[1], test_set_size, replace = FALSE)
#Check for standardization, even though glmnet inherently standardizes.
plot(as.ts(apply(x, 2, sd)), xlab="Covariate", ylab="Standard Deviation")
#Subset training data
x_training <- x[-test.index, ]
y_training <- y[-test.index]
#Subset the testing data
x_testing <- x[test.index, ]
y_testing <- y[test.index]
##-------------------##
### Cross Validation ###
##-------------------##
# --- Problem 1 - b) - i) ---- Performing cross validation with training data
lassoCV <- cv.glmnet(x_training, y_training, type.measure="mse", nfolds = 10, keep = TRUE, alpha =1)
#Svae the fold IDs to use for each method
std_foldid <- lassoCV$foldid
plot(lassoCV, label = FALSE)
par(mar=c(5, 4, 6, 2))
plot(lassoCV)
title("LASSO Cross-Validation Plot")
sps.set <- data.frame(lambda = lassoCV$lambda,nzero = lassoCV$nzero,mse = lassoCV$cvm) %>%
filter(lambda >= lassoCV$lambda.min & lambda <= lassoCV$lambda.1se) %>%
arrange(mse)
#Print table to screen
kable(sps.set)
#Plot the change in MSE specifically for our set of sps.set models, looking also at the number of nonzero coefficients (set to point size).
ggplot(sps.set, aes(x = log(lambda), y = mse)) +
geom_point(aes(size = nzero)) + #Size based on nzero values
geom_line() +
geom_point(data = sps.set[2, ], aes(x = log(lambda), y = mse), color = "blue", size = 8, shape = 8, alpha = 0.75) + #Highight alternative model choice, index 2 from the dataframe.
geom_text(aes(label = nzero), vjust = -0.8, hjust = 2, color = "darkgrey", size = 3.5) + #Label nzero values, moving them out of the way and colouring grey
geom_text(data = sps.set[2, ], aes(x = log(lambda), y = mse, label = sprintf("lambda = %.6f", lambda)), vjust = -5, color = "blue", nudge_y = 0.1) + #move label down to avoid overlap w nzero labels
scale_size_continuous(range = c(1, 6), guide = "none") + #Play around with size range for visibility of differences
theme_minimal() +
labs(title = "MSE vs.Lambda with # of Non-Zero Coefficients Labeled", x = "Log(Lambda)",y = "Mean Squared Error (mse)")
View(sps.set)
lassoCV$lambda
lassoCV$lambda.1se
sps.set <- data.frame(lambda = lassoCV$lambda,nzero = lassoCV$nzero,mse = lassoCV$cvm) %>%
filter(lambda >= lassoCV$lambda.min & lambda <= lassoCV$lambda.1se) %>%
arrange(mse) %>%
mutate(percent_diff = ((mse - lassoCV$lambda.min) /lassoCV$lambda.min) * 100) #Add column to show percentage difference for each MSE from the best model (lambda.min)
View(sps.set)
lassoCV$lambda.min
min(lassoCV$lambda.min)
min(lassoCV$cvm)
min_mse <- min(lassoCV$cvm)
sps.set <- data.frame(lambda = lassoCV$lambda,nzero = lassoCV$nzero,mse = lassoCV$cvm) %>%
filter(lambda >= lassoCV$lambda.min & lambda <= lassoCV$lambda.1se) %>%
arrange(mse) %>%
mutate(percent_diff = ((mse - min_mse) /min_mse) * 100) #Add column to show percentage difference for each MSE from the best model (lambda.min)
View(sps.set)
ggplot(sps.set, aes(x = log(lambda), y = percent_diff)) +
geom_point(aes(size = nzero)) + #Size based on nzero values
geom_line() +
geom_point(data = sps.set[2, ], aes(x = log(lambda), y = mse), color = "blue", size = 8, shape = 8, alpha = 0.75) + #Highlight alternative model choice, index 2 from the data frame.
geom_text(aes(label = nzero), vjust = -0.8, hjust = 2, color = "darkgrey", size = 3.5) + #Label nzero values, moving them out of the way and colouring grey
geom_text(data = sps.set[2, ], aes(x = log(lambda), y = mse, label = sprintf("lambda = %.6f", lambda)), vjust = -5, color = "blue", nudge_y = 0.1) + #move label down to avoid overlap w nzero labels
scale_size_continuous(range = c(1, 6), guide = "none") + #Play around with size range for visibility of differences
theme_minimal() +
labs(title = "MSE vs.Lambda with # of Non-Zero Coefficients Labeled", x = "Log(Lambda)",y = "Mean Squared Error (mse)")
ggplot(sps.set, aes(x = log(lambda), y = percent_diff)) +
geom_point(aes(size = nzero)) + #Size based on nzero values
geom_line() +
geom_point(data = sps.set[2, ], aes(x = log(lambda), y = percent_diff), color = "blue", size = 8, shape = 8, alpha = 0.75) + #Highlight alternative model choice, index 2 from the data frame.
geom_text(aes(label = nzero), vjust = -0.8, hjust = 2, color = "darkgrey", size = 3.5) + #Label nzero values, moving them out of the way and colouring grey
geom_text(data = sps.set[2, ], aes(x = log(lambda), y = mse, label = sprintf("lambda = %.6f", lambda)), vjust = -5, color = "blue", nudge_y = 0.1) + #move label down to avoid overlap w nzero labels
scale_size_continuous(range = c(1, 6), guide = "none") + #Play around with size range for visibility of differences
theme_minimal() +
labs(title = "MSE vs.Lambda with # of Non-Zero Coefficients Labeled", x = "Log(Lambda)",y = "Mean Squared Error (mse)")
ggplot(sps.set, aes(x = log(lambda), y = percent_diff)) +
geom_point(aes(size = nzero)) + #Size based on nzero values
geom_line() +
geom_point(data = sps.set[2, ], aes(x = log(lambda), y = percent_diff), color = "blue", size = 8, shape = 8, alpha = 0.75) + #Highlight alternative model choice, index 2 from the data frame.
geom_text(aes(label = nzero), vjust = -0.8, hjust = 2, color = "darkgrey", size = 3.5) + #Label nzero values, moving them out of the way and colouring grey
geom_text(data = sps.set[2, ], aes(x = log(lambda), y = percent_diff, label = sprintf("lambda = %.6f", lambda)), vjust = -5, color = "blue", nudge_y = 0.1) + #move label down to avoid overlap w nzero labels
scale_size_continuous(range = c(1, 6), guide = "none") + #Play around with size range for visibility of differences
theme_minimal() +
labs(title = "MSE vs.Lambda with # of Non-Zero Coefficients Labeled", x = "Log(Lambda)",y = "Mean Squared Error (mse)")
ggplot(sps.set, aes(x = log(lambda), y = percent_diff)) +
geom_point(aes(size = nzero), alpha = 0.75) + # Unified point layer with alpha for all points
geom_line() +
geom_point(data = sps.set[2, ], color = "blue", size = 8, shape = 8) + # Highlight alternative model choice
geom_text(aes(label = nzero), vjust = -0.8, hjust = 2, color = "darkgrey", size = 3.5) +
geom_text(data = sps.set[2, ], aes(label = sprintf("lambda = %.6f", lambda)), vjust = -5, color = "blue", nudge_y = 0.1) +
scale_size_continuous(range = c(1, 6), guide = "none") + # Adjust point sizes based on 'nzero', no legend for sizes
theme_minimal() +
labs(title = "Percent Difference vs. Log(Lambda) Highlighting Model Choices",
x = "Log(Lambda)",
y = "% Difference from Lambda.1se MSE")
